{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "crawlers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1eSqYi6UImgiEDSMfBllqrdGHGzqWxT_p",
      "authorship_tag": "ABX9TyNXzRWONPVbUVarjO5vujim",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olaviinha/SloppyNoto/blob/master/crawlers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV2Ur2lckLq5"
      },
      "source": [
        "#Data Crawlers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8Q5l2Ej13GK"
      },
      "source": [
        "\n",
        "This utility is to accompany [Sloppy Noto](https://github.com/olaviinha/SloppyNoto) to provide a bit of aid in the quest of hunting down possible data file candidates for data to audio conversion.\n",
        "\n",
        "<font color=\"#9d9\">**HTTP Crawler**</font> crawls ftp-like websites (_\"Index of /something\"_). Due to federal policies, some instances such as ESA and NASA have changed some of their data archive servers from FTP to HTTP(S). This crawler will take care of that. HTTP Crawler crawls 3 levels of subdirectories in depth.\n",
        "\n",
        "<font color=\"#99d\">**FTP Crawler**</font> crawls FTP directories as anonymous. FTP Crawler crawls **all** levels of subdirectories in depth, and it is **not recommended** to crawl top level, or any upper level directories (such as a directory containing _all data of an entire space mission_), for such crawling may take literally **hours** to complete.\n",
        "\n",
        "<font color=\"#9dd\">**ZIP Crawler**</font> crawls individual compressed archive files (.zip, .tar.gz). ZIP Crawler crawls all levels of subdirectories within the zip. To use data files from inside crawled zips, you must copy the data files to your Google Drive using the enclosed <font color=\"#9dd\">Drive Stasher</font> cell.\n",
        "\n",
        "**All of the crawlers** output:\n",
        "- **Recursive** list of all files over 20 MB in size from whatever you're crawling.\n",
        "- Categorized and sorted by file size.\n",
        "- File types of your choosing highlighted with a green arrow (<font color=\"#9d9\">`=>`</font>)\n",
        "\n",
        "**Howto**\n",
        "- Input ftp, http(s) or zip (.zip, .tar.gz) address to the corresponding crawler cell and run cell by clicking the play button on the left side of cell.\n",
        "- Copy/paste paths from the results to Sloppy Noto's `data_file` field.\n",
        "- To use data files found from inside crawled ZIP files, use <font color=\"#9dd\">Drive Stasher</font> cell to copy those files from ZIPs to your Google Drive. Then use the provided Drive path in Sloppy Noto.\n",
        "\n",
        "**Note**\n",
        "- Everything this utility does is **guessing** based on file names and sizes, thus all results are fully **indicative**. It is certainly not set in stone that any files that these crawlers find are going to work in Sloppy Noto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNWW748gbA3Z",
        "cellView": "form"
      },
      "source": [
        "#@title Setup & Settings\n",
        "\n",
        "#@markdown <small>Highlight these extensions in all crawl results. Case-insensitive, comma-separated list of file extensions, including period.</small>\n",
        "highlight_extensions = \".csv, .tsv, .tab, .lst, .log\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "#@markdown <small>Path to a directory in your Google Drive. Everything you choose to save, will be saved in this directory. Relative to your Drive root.</small>\n",
        "save_dir = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown <small>Save resulting file lists from your crawls as well as source information of copied files as .txt files (to the directory set above). May come in handy in the future, not having to crawl the same things time after time.</small>\n",
        "save_output_txt = False #@param {type:\"boolean\"}\n",
        "\n",
        "save_txt = save_output_txt\n",
        "\n",
        "\n",
        "from google.colab import output\n",
        "import os\n",
        "\n",
        "force_setup = False\n",
        "\n",
        "  \n",
        "# inhagcutils\n",
        "if not os.path.isfile('/content/inhagcutils.ipynb') and force_setup == False:\n",
        "  pip_packages = 'ftputil'\n",
        "  %cd /content/\n",
        "  !pip -q install import-ipynb {pip_packages}\n",
        "  !curl -s -O https://raw.githubusercontent.com/olaviinha/inhagcutils/master/inhagcutils.ipynb\n",
        "# Mount Drive\n",
        "if not os.path.isdir('/content/drive') and force_setup == False:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "# Drive symlink\n",
        "if not os.path.isdir('/content/mydrive') and force_setup == False:\n",
        "  os.symlink('/content/drive/My Drive', '/content/mydrive')\n",
        "  drive_root_set = True\n",
        "\n",
        "import import_ipynb\n",
        "from inhagcutils import *\n",
        "drive_root = '/content/mydrive/'\n",
        "dir_tmp = '/content/tmp/'\n",
        "if not os.path.isdir(dir_tmp):\n",
        "  create_dirs([dir_tmp])\n",
        "\n",
        "import sys\n",
        "import ftplib\n",
        "import ftputil\n",
        "import fnmatch\n",
        "\n",
        "\n",
        "#size_limits = [1000, 500, 200, 100, 50, 20]\n",
        "size_limits = [1000, 500, 200, 100, 50, 20]\n",
        "b = 1000000\n",
        "size_limits = [limit*b for limit in size_limits]\n",
        "\n",
        "def fix_extensions(input_extensions):\n",
        "  extensions = input_extensions.split(',')\n",
        "  gz_extensions = [ext+'.gz' for ext in extensions]\n",
        "  extensions.extend(gz_extensions)\n",
        "  extensions = [ext.lower() for ext in extensions]\n",
        "  extensions.extend([ext.upper() for ext in extensions])\n",
        "  return tuple(extensions)\n",
        "\n",
        "def apnd(content):\n",
        "  global log_all, txt\n",
        "  log_all = open(txt, 'a+')\n",
        "  log_all.write(content)\n",
        "  log_all.close()\n",
        "\n",
        "def print_filelist(list, title, ftp=True):\n",
        "  global log_all, save_txt\n",
        "  total = len(list)\n",
        "  op(c.title, '\\n'+title+':\\n')\n",
        "  if save_txt == True:\n",
        "    apnd('\\n'+title+'\\n')\n",
        "  list.sort(key=lambda tup: tup[0], reverse=True)\n",
        "  i = 0\n",
        "  for s, f in list:\n",
        "    if ftp == True:\n",
        "      line = str('{:.2f}'.format(round(s/b, 2)))+' MB: ftp://'+ftp_host+f\n",
        "    else:\n",
        "      line = str('{:.2f}'.format(round(s/b, 2)))+' MB: '+f\n",
        "    s_line = line\n",
        "    if f.endswith(extensions):\n",
        "      if i < 500:\n",
        "        op(c.ok, '=>', line)\n",
        "      s_line = '=> '+line\n",
        "    else:\n",
        "      if i < 500:\n",
        "        print(line)\n",
        "    if i == 500:\n",
        "      remaining = total-500\n",
        "      print('...and', str(remaining), 'more files of this size scale were found.')\n",
        "      op(c.warn, '\\nPrinting was stopped at 500.', 'Full file list is saved in a .txt file if you have save_txt checked.')\n",
        "    if save_txt == True:\n",
        "      apnd(s_line+'\\n')\n",
        "    i += 1\n",
        "      \n",
        "def retrieve_print_filelist(basedir, ftp=True):\n",
        "  global size_limits, b\n",
        "  all_files = []\n",
        "  files_by_size = [[] for _ in size_limits]\n",
        "\n",
        "  if ftp == True:\n",
        "    recursive = host.walk(basedir, topdown=True, onerror=None) # recursive search \n",
        "  else:\n",
        "    recursive = os.walk(basedir, topdown=True)\n",
        "  print('Recursive file list retrieval in progress...')\n",
        "  files_found = 0\n",
        "  for root, dirs, files in recursive:\n",
        "    for name in files:\n",
        "      fullpath = os.path.join(root, name)\n",
        "      if ftp == True:\n",
        "        size = host.path.getsize(fullpath)\n",
        "      else:\n",
        "        size = os.path.getsize(fullpath)\n",
        "      all_files.append([size, fullpath])\n",
        "\n",
        "      for i, limit in enumerate(size_limits):\n",
        "        if i == 0:\n",
        "          if size > limit:\n",
        "            files_by_size[i].append([size, fullpath])\n",
        "            files_found += 1\n",
        "        else:\n",
        "          if size > limit and size < size_limits[i-1]:\n",
        "            files_by_size[i].append([size, fullpath])\n",
        "            files_found += 1\n",
        "  output.clear()\n",
        "  op(c.ok, '\\nResults\\n\\n')\n",
        "\n",
        "  if files_found == 0:\n",
        "    op(c.fail, 'No suitable files found.')\n",
        "  else:\n",
        "    for i, filelist in enumerate(files_by_size):\n",
        "      if len(filelist) > 0:\n",
        "        if i == 0:\n",
        "          title = 'Over 1 GB'\n",
        "        else:\n",
        "          title = str(round(size_limits[i]/b))+'-'+str(round(size_limits[i-1]/b))+' MB'\n",
        "        print_filelist(filelist, title, ftp=ftp)\n",
        "    if save_txt == True:\n",
        "      log_all.close()\n",
        "\n",
        "extensions = fix_extensions(highlight_extensions)\n",
        "save_path = fix_path(drive_root+save_dir)\n",
        "\n",
        "output.clear()\n",
        "op(c.ok, 'Setup finished.')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6Au5rsofT9o"
      },
      "source": [
        "#<font color=\"#9d9\">HTTP Crawler</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNoDV2YHfXPq",
        "cellView": "form"
      },
      "source": [
        "#@markdown HTTP Crawler is meant strictly for http(s) addresses with an FTP-like view (normally titled `Index of /something`).\n",
        "#@markdown Crawls **3 levels** deep in the file system tree structure.\n",
        "web_url = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#import pprint from pprint\n",
        "import requests\n",
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "if save_txt == True:\n",
        "  web_host = slug(web_url.replace('http://', '').replace('https://', '').split('/')[0])\n",
        "  webdir = web_url.replace(webhost, '').replace('/', '_')\n",
        "  txt = save_path+'filelist_'+webhost+'_'+path_leaf(web_url)+'_'+rnd_str(4)+'.txt'\n",
        "  apnd('WEB URL: '+web_url+'\\n')\n",
        "  apnd('WEB host: '+web_host+'\\n')\n",
        "\n",
        "def get_url_paths(url, ext='', params={}):\n",
        "  response = requests.get(url, params=params)\n",
        "  if response.ok:\n",
        "    response_text = response.text\n",
        "  else:\n",
        "    return response.raise_for_status()\n",
        "  soup = BeautifulSoup(response_text, 'html.parser')\n",
        "  \n",
        "  #link = [url + node.get('href') for node in soup.find_all('a') if node.get('href').endswith(ext)]\n",
        "  #parent = [url + node.get('href') for node in soup.find_all('a') if node.get('href').endswith(ext)]\n",
        "  li = []\n",
        "  for link in soup.find_all('a'):\n",
        "    href = link.get('href')\n",
        "    if href != '.' and href != '..':\n",
        "      fullhref = url + href\n",
        "      meta = link.next_sibling.split()\n",
        "      if len(meta) > 0:\n",
        "        size = meta[2]\n",
        "      else:\n",
        "        size = 0\n",
        "      \n",
        "      #print('blaa', blaa)\n",
        "      #print(date)\n",
        "      #print(size)\n",
        "      li.append([size, fullhref])\n",
        "  return li\n",
        "  #return parent\n",
        "\n",
        "def crawl_dirs(links):\n",
        "  dir_contents = []\n",
        "  ndx = 0\n",
        "  for i, link in enumerate(links):\n",
        "    href = link[1]\n",
        "    if href.endswith('/'):\n",
        "      if not href.endswith('../') and not href.endswith('./'):\n",
        "        #dir_contents.append([])\n",
        "        #dir_contents[ndx] = get_url_paths(link)\n",
        "        dir_contents.append( get_url_paths(href) )\n",
        "        ndx += 1\n",
        "  return dir_contents\n",
        "\n",
        "\n",
        "links = get_url_paths(web_url, '')\n",
        "subs = crawl_dirs(links)\n",
        "\n",
        "subsubs = []\n",
        "for i, sub in enumerate(subs):\n",
        "  subsubs.append(crawl_dirs(sub))\n",
        "\n",
        "def size_filter(links):\n",
        "  global size_limits, b\n",
        "  files_by_size = [[] for _ in size_limits]\n",
        "  files_found = 0\n",
        "  for link in links:\n",
        "    size = link[0]\n",
        "    if size != '-':\n",
        "      size = int(size)\n",
        "    href = link[1]\n",
        "    if not href.endswith('/'):\n",
        "      fullpath = href\n",
        "      for i, limit in enumerate(size_limits):\n",
        "        if i == 0:\n",
        "          if size > limit:\n",
        "            files_by_size[i].append([size, fullpath])\n",
        "            files_found += 1\n",
        "        else:\n",
        "          if size > limit and size < size_limits[i-1]:\n",
        "            files_by_size[i].append([size, fullpath])\n",
        "            files_found += 1\n",
        "      #print(size, href)\n",
        "      #print(get_size(link))\n",
        "\n",
        "  output.clear()\n",
        "  op(c.ok, '\\nResults\\n\\n')\n",
        "  if files_found == 0:\n",
        "    op(c.fail, 'No suitable files found.')\n",
        "  else:\n",
        "    for i, filelist in enumerate(files_by_size):\n",
        "      if len(filelist) > 0:\n",
        "        if i == 0:\n",
        "          title = 'Over 1 GB'\n",
        "        else:\n",
        "          title = str(round(size_limits[i]/b))+'-'+str(round(size_limits[i-1]/b))+' MB'\n",
        "        print_filelist(filelist, title, ftp=False)\n",
        "\n",
        "\n",
        "all_fucking_links = []\n",
        "\n",
        "#lvl 1\n",
        "if len(links) > 0:\n",
        "  #size_filter(links)\n",
        "  all_fucking_links.extend(links)\n",
        "\n",
        "#lvl 2\n",
        "if len(subs) > 0:\n",
        "  for sub in subs:\n",
        "    #size_filter(sub)\n",
        "    all_fucking_links.extend(sub)\n",
        "\n",
        "#lvl 3\n",
        "if len(subsubs) > 0:\n",
        "  for subsub in subsubs:\n",
        "    for subsubst in subsub:\n",
        "      #size_filter(subsubst)\n",
        "      all_fucking_links.extend(subsubst)\n",
        "\n",
        "#print( all_fucking_links[0] )\n",
        "size_filter(all_fucking_links)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRysml1cThqT"
      },
      "source": [
        "#<font color=\"#99d\">FTP Crawler</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PHcOXUY5X6W",
        "cellView": "form"
      },
      "source": [
        "#@markdown FTP Crawler should not be used on upper level directories, or it may take **hours** to complete the listing. \n",
        "#@markdown Crawls to infinite depths in the file system tree structure.\n",
        "\n",
        "ftp_url = \"\" #@param {type:\"string\"}\n",
        "\n",
        "ftp_address = ftp_url.replace('ftp://', '')\n",
        "ftp_host = ftp_address.split('/')[0]\n",
        "basedir = ftp_address.replace(ftp_host, '')\n",
        "\n",
        "if save_txt == True:\n",
        "  txt = save_path+'filelist_'+ftp_host+'_'+rnd_str(4)+'.txt'\n",
        "  apnd('FTP URL: '+ftp_url+'\\n')\n",
        "  apnd('FTP host: '+ftp_host+'\\n')\n",
        "  apnd('FTP dir: '+basedir+'\\n\\n')\n",
        "output.clear()\n",
        "op(c.title, 'Logging in to '+ftp_host+'...')\n",
        "host = ftputil.FTPHost(ftp_host, 'anonymous', 'anonymous@domain.com')\n",
        "\n",
        "retrieve_print_filelist(basedir, ftp=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCpr03phTRHJ"
      },
      "source": [
        "#<font color=\"#9dd\">ZIP Crawler</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aQascEM-EP6",
        "cellView": "form"
      },
      "source": [
        "#@markdown ZIP Crawler is to be used for compressed files (`.zip`, `.gz`, `.tar.gz`). Crawls to infinite depths within the zip's enclosing tree structure.\n",
        "zip_url = \"\" #@param {type:\"string\"}\n",
        "\n",
        "if save_txt == True:\n",
        "  zipfile = basename(zip_url)\n",
        "  txt = save_path+'filelist_zip_'+zipfile+'_'+rnd_str(4)+'.txt'\n",
        "  apnd('ZIP URL: '+zip_url+'\\n\\n')\n",
        "\n",
        "def is_zipzip(path):\n",
        "  return path_ext(path).lower() == '.zip'\n",
        "  \n",
        "def is_gz(path):\n",
        "  return path_ext(path).lower() == '.gz'\n",
        "\n",
        "zip_ext = path_ext(zip_url, True)\n",
        "#id = rnd_str(6)\n",
        "wfile = slug(basename(zip_url))\n",
        "wdir = dir_tmp+wfile+'/'\n",
        "wext = path_ext(zip_url)\n",
        "file_path = wdir+wfile+wext\n",
        "if not os.path.isdir(wdir):\n",
        "  !mkdir {wdir}\n",
        "  !wget {zip_url} -O {file_path}\n",
        "  if is_gz(file_path):\n",
        "    if '.tar.gz' in path_leaf(file_path):\n",
        "      !tar xvzf {file_path}\n",
        "    else:\n",
        "      !gunzip {file_path}\n",
        "  elif is_zipzip(file_path):\n",
        "    %cd {wdir}\n",
        "    !unzip {file_path}\n",
        "    %cd /content/\n",
        "  !rm {wdir}{wfile}{wext}\n",
        "\n",
        "retrieve_print_filelist(wdir, ftp=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYynycjS1Y8d"
      },
      "source": [
        "##<font color=\"#9dd\">Drive Stasher</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv6X-rx3NAXV",
        "cellView": "form"
      },
      "source": [
        "#@markdown Use this cell to stash data files from ZIP Crawler to your Drive.<br>\n",
        "#@markdown <small>Paste file path and run cell.</small>\n",
        "copy_file = \"\" #@param {type:\"string\"}\n",
        "\n",
        "!cp {copy_file} {save_path}\n",
        "op(c.ok, 'Copied to Drive:', path_leaf(copy_file))\n",
        "print('You may now use this file in Sloppy Noto. Set data_file:', copy_file)\n",
        "if save_txt == True:\n",
        "  apnd('\\nCopied '+path_leaf(copy_file)+' to '+save_path)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}